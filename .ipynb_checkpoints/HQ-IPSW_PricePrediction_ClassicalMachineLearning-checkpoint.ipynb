{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features with Pearson Correlation: ['Toronto ED', 'SouthWest ED', 'Ontario ED', 'Essa PD', 'Ottawa PD', 'Essa ED', 'Weekly Highest Minimum Demand  Ontario', 'Ontario PD', 'Ottawa ED', 'West ED']\n",
      "Selected features with MI: ['Expected Hydro Output', 'Normal Peak Temperature (°C)', 'Expected Wind Output', 'Available Nuclear and Wind Dispatch', 'SouthWest PD', 'Export Assumption', 'Ontario ED', 'Niagara ED', 'Essa PD', 'Ottawa PD']\n",
      "Selected features with Lasso: ['Bruce PD' 'NorthEast PD' 'NorthWest PD' 'Load Forecast Uncertainty'\n",
      " 'Essa ED' 'NorthEast ED' 'SouthWest ED' 'Toronto ED'\n",
      " 'Baseload Generation after Exports and Nuclear and Wind Dispatch MW.1'\n",
      " 'Lowest Weekly SBG after Exports']\n",
      " \n",
      "Number of selected features:  24\n",
      "Selected features:\n",
      "['Toronto ED', 'SouthWest ED', 'Ontario ED', 'Essa PD', 'Ottawa PD', 'Essa ED', 'Weekly Highest Minimum Demand  Ontario', 'Ontario PD', 'Ottawa ED', 'West ED', 'Expected Hydro Output', 'Normal Peak Temperature (°C)', 'Expected Wind Output', 'Available Nuclear and Wind Dispatch', 'SouthWest PD', 'Export Assumption', 'Niagara ED', 'Bruce PD', 'NorthEast PD', 'NorthWest PD', 'Load Forecast Uncertainty', 'NorthEast ED', 'Baseload Generation after Exports and Nuclear and Wind Dispatch MW.1', 'Lowest Weekly SBG after Exports']\n",
      " \n",
      "\n",
      "rMSE for validation set:\n",
      "Linear regression rMSE: 7.947844114008142\n",
      "Lasso rMSE for validation set: 7.977928202429075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/ai.mila.quebec/apps/x86_64/debian/anaconda/3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic net rMSE: 7.932837748865415\n",
      "Gradient Boosting rMSE on validation set: 5.1791\n",
      "MLP rMSE: 8.9153\n",
      "Test is done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import pearsonr\n",
    "import glob\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LogisticRegression, LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import datasets, ensemble\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LogisticRegression, LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Parameters\n",
    "path = \"Data_18months_Outlook/*/*.xlsx\"\n",
    "glob.glob(path)\n",
    "\n",
    "K_corr = 10 # Number of selected features with correlation\n",
    "K_mi = 10 # Number of features selected with MI\n",
    "printFeat = True\n",
    "plotFig = False \n",
    "N_train = 19\n",
    "\n",
    "\n",
    "## Ascending format\n",
    "#ascending_order = list([18,5,8,17,6,10,3,4,15,1,21,11,20,16,19,2,14,13,12,7,0,9])\n",
    "#ascend_list = [glob.glob(path)[i] for i in ascending_order]\n",
    "#my_list = ascend_list\n",
    "\n",
    "train_test_order = list([18,8,17,6,10,3,4,15,1,21,11,16,2,14,13,12,7,0,9, 5,19,20])\n",
    "my_list = [glob.glob(path)[i] for i in train_test_order]\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------\n",
    "#                                    TRAINING\n",
    "#--------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "for f in my_list[0:N_train]:\n",
    "    df = pd.read_excel(f)\n",
    "    train_data = train_data.append(df,ignore_index=True)\n",
    "# Converting datetime\n",
    "train_data['Date (week ending)'] = pd.to_datetime(train_data['Date (week ending)'])\n",
    "\n",
    "\n",
    "## ----------------------------------------------------\n",
    "# Drop rows with missing values in selected columns\n",
    "## ----------------------------------------------------\n",
    "# train data\n",
    "df = train_data.dropna(axis = 0, how='any', subset=['HOEP'])\n",
    "df = df.dropna(axis = 0, how='any', subset=['Baseload Generation'])\n",
    "\n",
    "# --------------------------------\n",
    "# Pearson Correlation analysis\n",
    "# -------------------------------\n",
    "corr_list = []\n",
    "hoep = df['HOEP']\n",
    "columns_name = df.columns.ravel()\n",
    "\n",
    "for i in columns_name[2:-2]:       # last two feature which are almost always zero are not included in the correlation to prevent warning for the code\n",
    "    param = df[i]\n",
    "    nans = np.isnan(np.array(param))\n",
    "    corr, _ = pearsonr(hoep[~nans], param[~nans])\n",
    "    corr_list.append(np.abs(corr))\n",
    "\n",
    "corr_dictionary = dict(zip(columns_name[2:-2], corr_list))\n",
    "sorted_corr = {k: v for k, v in sorted(corr_dictionary.items(), key=lambda item: item[1], reverse=True)}\n",
    "corr_ind_dictionary = dict(zip(np.add(range(len(columns_name)-2),2), corr_list))\n",
    "sorted_ind_corr = {k: v for k, v in sorted(corr_dictionary.items(), key=lambda item: item[1], reverse=True)}\n",
    "inds_sorted = list(sorted_ind_corr.keys()) \n",
    "\n",
    "# Selected features with Correlation\n",
    "sel_feat_corr = inds_sorted[0:K_corr]\n",
    "\n",
    "if printFeat:    \n",
    "    print('Selected features with Pearson Correlation:', sel_feat_corr)\n",
    "\n",
    "if plotFig:\n",
    "    # Plot the correlations\n",
    "    keys = list(sorted_corr.keys()) \n",
    "    values = list(sorted_corr.values()) \n",
    "    fig = plt.figure(figsize = (10, 5)) \n",
    "    # creating the bar plot \n",
    "    plt.bar(keys, values, color ='maroon') \n",
    "    plt.xlabel(\"Variables\") \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Pearson correlation coefficient\") \n",
    "    plt.title(\"Correlation of each parameter to HOEP\") \n",
    "    plt.savefig('Result_files/Figures/Correlation.png',bbox_inches='tight')\n",
    "    plt.show() \n",
    "\n",
    "# -------------------------------\n",
    "# Mutual Information\n",
    "# -------------------------------\n",
    "keys = list(sorted_corr.keys()) \n",
    "X = np.array(df[0:][keys])\n",
    "Y = df[0:]['HOEP']\n",
    "\n",
    "mi = mutual_info_regression(X, Y)\n",
    "mi /= np.max(mi)\n",
    "\n",
    "ind_sort = np.argsort(-mi)\n",
    "mi_dictionary = dict(zip(columns_name[2:-1], mi))\n",
    "sorted_mi = {k: v for k, v in sorted(mi_dictionary.items(), key=lambda item: item[1], reverse=True)}\n",
    "mi_ind_dictionary = dict(zip(np.add(range(len(columns_name)-2),2), mi))\n",
    "sorted_ind_mi = {k: v for k, v in sorted(mi_dictionary.items(), key=lambda item: item[1], reverse=True)}\n",
    "inds_sorted = list(sorted_ind_mi.keys()) \n",
    "\n",
    "# Selected features with MI\n",
    "sel_feat_mi = inds_sorted[0:K_mi]\n",
    "if printFeat:\n",
    "    print('Selected features with MI:', sel_feat_mi)\n",
    "\n",
    "if plotFig:\n",
    "    # Plot the Modulation index\n",
    "    keys = list(sorted_mi.keys()) \n",
    "    values = list(sorted_mi.values()) \n",
    "    fig = plt.figure(figsize = (10, 5)) \n",
    "    # creating the bar plot \n",
    "    plt.bar(keys, values, color ='indigo') \n",
    "    plt.xlabel(\"Variables\") \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"MI\") \n",
    "    plt.title(\"Mutual information of HOEP and variables\") \n",
    "    plt.savefig('Result_files/Figures/MI.png',bbox_inches='tight')\n",
    "    plt.show() \n",
    "# --------------------------------------------------\n",
    "# Feature selection using regularization with LASSO \n",
    "# --------------------------------------------------\n",
    "\n",
    "# Train-val split\n",
    "X_train_full, X_val_full, y_train, y_val = train_test_split(\n",
    "    df.drop(labels=['HOEP'], axis=1),\n",
    "    df['HOEP'],\n",
    "    test_size=0.25,\n",
    "    random_state=0)\n",
    "\n",
    "X_train = X_train_full.drop(labels=['Date (week ending)'], axis=1)\n",
    "X_val = X_val_full.drop(labels=['Date (week ending)'], axis=1)\n",
    "\n",
    "columns_name = df.columns.ravel()\n",
    "Features_name = columns_name[2:]\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# here, again I will train a Lasso Linear regression and select\n",
    "# the non zero features in one line.\n",
    "\n",
    "# Regression task\n",
    "sel_ = SelectFromModel(Lasso(alpha=.35))\n",
    "sel_.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "# make a list with the selected features and print the outputs\n",
    "selected_feat = Features_name[(sel_.get_support())]\n",
    "sel_feat_index = np.array(sel_.get_support())\n",
    "sel_feat_lasso = Features_name[(sel_.get_support())]\n",
    "if printFeat:\n",
    "    print('Selected features with Lasso:', sel_feat_lasso)\n",
    "\n",
    "\n",
    "# Concatenate selected features\n",
    "sel_feat_concat = list(np.concatenate((sel_feat_corr, sel_feat_mi, sel_feat_lasso)))\n",
    "sel_feat = []              # removing duplicates\n",
    "[sel_feat.append(x) for x in sel_feat_concat if x not in sel_feat] \n",
    "print(' ')\n",
    "print(\"Number of selected features: \", len(sel_feat))\n",
    "print('Selected features:')\n",
    "print(sel_feat)\n",
    "print(' \\n')\n",
    "\n",
    "# Transfer features to their index\n",
    "sel_feat_ind = []\n",
    "for feature in sel_feat:\n",
    "    sel_feat_ind.append(list(columns_name[2:]).index(feature))\n",
    "\n",
    "X_train = X_train[:,sel_feat_ind]\n",
    "X_val = X_val[:,sel_feat_ind]\n",
    "\n",
    "# --------------------------------\n",
    "# Linear Regression\n",
    "# -------------------------------\n",
    "\n",
    "reg_lin=LinearRegression()\n",
    "reg_lin.fit(X_train,y_train)\n",
    "rmse_Linear=np.sqrt(mean_squared_error(y_true=y_val,y_pred=reg_lin.predict(X_val)))\n",
    "print('rMSE for validation set:')\n",
    "print('Linear regression rMSE:', rmse_Linear)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Regression with Lasso\n",
    "# -----------------------------\n",
    "# Regression\n",
    "reg_lasso = LassoCV(cv=5, random_state=0, max_iter = 30000).fit(X_train, y_train)\n",
    "reg_lasso.score(X_train, y_train)\n",
    "y_pred = reg_lasso.predict(X_val)\n",
    "Lasso_rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print('Lasso rMSE for validation set:',Lasso_rmse)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Regression with Elastic Net (Both Ridge and Lasso)\n",
    "# ------------------------------------------\n",
    "elastic=ElasticNet(normalize=True, max_iter = 10000)\n",
    "\n",
    "# Finding the best parameters for the model\n",
    "search=GridSearchCV(estimator=elastic,param_grid={'alpha':np.logspace(-5,2,8),'l1_ratio':[.2,.4,.6,.8]},scoring='neg_mean_squared_error',n_jobs=1,refit=True,cv=10)\n",
    "search.fit(X_train,y_train)\n",
    "parameters_n = search.best_params_\n",
    "param_list = list(parameters_n.keys())\n",
    "\n",
    "# set the parameters\n",
    "elastic=ElasticNet(normalize=True,alpha=parameters_n[param_list[0]],l1_ratio=parameters_n[param_list[1]], max_iter = 30000)\n",
    "elastic.fit(X_train,y_train)\n",
    "elastic_rmse=np.sqrt(mean_squared_error(y_true=y_val,y_pred=elastic.predict(X_val)))\n",
    "print('Elastic net rMSE:', elastic_rmse)\n",
    "\n",
    "# --------------------------\n",
    "# Gradient boosting\n",
    "# -------------------------\n",
    "# Gradient boosting with the hyper parameters that are set in a separate process\n",
    "\n",
    "params = {'n_estimators': 600,\n",
    "          'max_depth': 6,\n",
    "          'min_samples_split': 5,\n",
    "          'learning_rate': 0.06,\n",
    "          'loss': 'ls'}\n",
    "reg_gb = ensemble.GradientBoostingRegressor(**params)\n",
    "reg_gb.fit(X_train, y_train)\n",
    "gb_rmse = np.sqrt(mean_squared_error(y_val, reg_gb.predict(X_val)))\n",
    "print(\"Gradient Boosting rMSE on validation set: {:.4f}\".format(gb_rmse))\n",
    "\n",
    "gb_fi = reg_gb.feature_importances_    # feature importance\n",
    "ind_sort = np.argsort(-gb_fi)\n",
    "gb_dictionary = dict(zip(sel_feat[0:-1], gb_fi))\n",
    "sorted_gb = {k: v for k, v in sorted(gb_dictionary.items(), key=lambda item: item[1], reverse=True)}\n",
    "gb_ind_dictionary = dict(zip(np.add(range(len(sel_feat)-2),2), gb_fi))\n",
    "sorted_ind_gb = {k: v for k, v in sorted(gb_dictionary.items(), key=lambda item: item[1], reverse=True)}\n",
    "inds_sorted = list(sorted_ind_gb.keys()) \n",
    "\n",
    "if plotFig:\n",
    "    keys = list(sorted_gb.keys()) \n",
    "    values = list(sorted_gb.values()) \n",
    "    fig = plt.figure(figsize = (10, 5)) \n",
    "    # creating the bar plot \n",
    "    plt.bar(keys, values, color ='darkcyan') \n",
    "    #plt.xlabel(\"Variables\") \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"Parameter's importance\") \n",
    "    plt.title(\"Parameters sorted with GB algorithm\") \n",
    "    plt.savefig('Result_files/Figures/GB.png',bbox_inches='tight')\n",
    "    plt.show() \n",
    "\n",
    "# --------------------------\n",
    "# MLP\n",
    "# -------------------------\n",
    "reg_mlp = MLPRegressor(random_state=1,hidden_layer_sizes=(10,3), max_iter=10000,learning_rate_init=.001,activation='relu')\n",
    "reg_mlp.fit(X_train, y_train)\n",
    "y_pred = reg_mlp.predict(X_val)\n",
    "#regr.score(X_val, y_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, reg_mlp.predict(X_val)))\n",
    "print(\"MLP rMSE: {:.4f}\".format(rmse))\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------\n",
    "#                                   FUNCTIONS\n",
    "#--------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# This function estimate monthly cost from weekly data with bringing them to daily system first    \n",
    "def estimate_monthly_cost(weekly_HOEP, weekly_datetime):   \n",
    "    weekly_dict = dict(zip(weekly_datetime, weekly_HOEP))    \n",
    "    daily_dict = {}\n",
    "    for key in weekly_dict:\n",
    "        for days_to_subtract in range(7):\n",
    "            new_key = key - timedelta(days=days_to_subtract)\n",
    "            daily_dict.update({new_key: weekly_dict[key]})\n",
    "\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df['Date'] = list(daily_dict.keys())\n",
    "    output_df['Date'] = pd.to_datetime(output_df['Date'])\n",
    "    output_df['HOEP'] = list(daily_dict.values())\n",
    "    return output_df.groupby(output_df.Date.dt.to_period(\"M\")).mean()\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------\n",
    "#                                    TESTING\n",
    "#--------------------------------------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------\n",
    "Full_path = \"Data_18months_Outlook/*/*.xlsx\"\n",
    "\n",
    "for i_test in range(3):\n",
    "    \n",
    "    test_data = pd.DataFrame()\n",
    "    for f in  my_list[i_test+N_train:i_test+N_train+1]:\n",
    "        df = pd.read_excel(f)\n",
    "        test_data = test_data.append(df,ignore_index=True)\n",
    "    \n",
    "        test_data['Date (week ending)'] = pd.to_datetime(test_data['Date (week ending)'])\n",
    "\n",
    "        # extracting the start of 18 month period\n",
    "        String = ''.join(f)\n",
    "        ind = String.find('.xlsx')\n",
    "        Range_name = String[ind-8:ind]\n",
    "        \n",
    "        # test data\n",
    "        df_test = test_data.dropna(axis = 0, how='any', subset=['East PD','Baseload Generation'])\n",
    "        df_test.describe()\n",
    "        \n",
    "        X_test = df_test[columns_name[2:]] # Removing price and date\n",
    "        X_test = np.array(X_test)\n",
    "        scaler = StandardScaler()          # Scaling the data\n",
    "        scaler.fit(X_test)\n",
    "\n",
    "        X_test = X_test[:,sel_feat_ind]    # Extracting selected features\n",
    "\n",
    "        # Gradient boosting\n",
    "        pred_gb_test_df = estimate_monthly_cost(reg_gb.predict(X_test), df_test['Date (week ending)'])\n",
    "        a = str('Result_files/'+str(Range_name)+'_gb_result.xlsx')\n",
    "        pred_gb_test_df.to_excel(a)\n",
    "\n",
    "        # Linear regression\n",
    "        pred_other_test_df = estimate_monthly_cost(reg_lin.predict(X_test), df_test['Date (week ending)'])\n",
    "        b = str('Result_files/'+str(Range_name)+'_lin_result.xlsx')\n",
    "        pred_other_test_df.to_excel(b)\n",
    "\n",
    "        # Lasso regression\n",
    "        pred_other_test_df = estimate_monthly_cost(reg_lasso.predict(X_test), df_test['Date (week ending)'])\n",
    "        b = str('Result_files/'+str(Range_name)+'_lasso_result.xlsx')\n",
    "        pred_other_test_df.to_excel(b)\n",
    "\n",
    "print('Test is done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (condaEnv)",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
